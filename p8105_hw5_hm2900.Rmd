---
title: "P8105_hw5_hm2900"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load data files.

```{r message = FALSE}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

Tidy the dataframe through using string manipulations on the file, converting from wide to long, and selecting relevant variables.

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way.


## Problem 2

Describe the raw data. Create a `city_state` variable, and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r message = FALSE}
homicide_df = 
  read_csv("./data/homicide-data.csv", na = c("", "Unknown")) %>%
  mutate(
    city_state = str_c(city, ", ", state),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved", 
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )
  ) %>%
  relocate(city_state) %>%
  filter(city_state != "Tulsa, AL")

distinct_city = 
  homicide_df %>%
  distinct(city_state)

cities_df = homicide_df %>%
  group_by(city_state) %>%
  summarize(
    unsolved = sum(resolution == "unsolved"),
    total = n()
  )

cities_df
```
The data contains `r nrow(homicide_df)` criminal homicides over the past decade in `r nrow(distinct_city)` of the largest American cities. The data included the location of the killing, reported date, basic information of the victim (name, race, age, sex), whether an arrest was made and, in most cases, basic demographic information about each victim.

For the city of Baltimore, MD, estimate the proportion of homicides that are unsolved.

```{r message = FALSE}
baltimore_md_df = 
  homicide_df %>%
  filter(city_state == "Baltimore, MD")

baltimore_md_summary = 
  baltimore_md_df %>%
  summarize(
    unsolved = sum(resolution == "unsolved"),
    total = n()
  )

baltimore_md_test = 
  prop.test(
    x = baltimore_md_summary %>% pull(unsolved),
    n = baltimore_md_summary %>% pull(total)
  ) %>%
  broom::tidy() %>%
  select(estimate, conf.low, conf.high)

baltimore_md_test
```


Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each.

```{r message = FALSE}
all_df = 
  cities_df %>%
  mutate(
    test_results = map2(unsolved, total, prop.test),
    tidy_results = map(test_results, broom::tidy)
  ) %>%
  select(city_state, tidy_results) %>%
  unnest(tidy_results) %>%
  select(city_state, estimate, conf.low, conf.high)

all_df
```


Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
all_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


## Problem 3

First set the following design elements:

```{r}
sim_normal_mean = function(n_obs = 30, mu, sigma = 5) {
  
  x = rnorm(n = n_obs, mean = mu, sd = sigma)
  
  x

}
```

Set μ=0. Generate 5000 datasets from the model. Save μ^ and the p-value arising from a test of H:μ=0 using α=0.05.

```{r}
sim_results_df = 
  expand_grid(
    mean_value = 0,
    interation = 1:5000
  ) %>%
  mutate(
    vect_num = map(.x = mean_value, ~sim_normal_mean(mu = .x))
  ) %>%
  mutate(
    estimate_df = map(vect_num, t.test),
    clean_output = map(estimate_df, broom::tidy) 
  ) %>%
  unnest(clean_output) %>%
  select(estimate, p.value)

sim_results_df
```

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

```{r}
sim_results_changes_df = 
  expand_grid(
    mean_value = c(1, 2, 3, 4, 5, 6),
    interation = 1:5000
  ) %>%
  mutate(
    vect_num = map(.x = mean_value, ~sim_normal_mean(mu = .x))
  ) %>%
  mutate(
    estimate_df = map(vect_num, t.test),
    clean_output = map(estimate_df, broom::tidy) 
  ) %>%
  unnest(clean_output) %>%
  select(mean_value, estimate, p.value)
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.

```{r}
sim_results_changes_df %>%
  mutate(
    rejection = case_when(
      p.value < 0.05 ~ 1, 
      p.value > 0.05 ~ 0
    )
  ) %>%
  group_by(mean_value) %>%
  summarize(
    reject = sum(rejection == 1),
    total = n(),
    proportion = reject / total
  ) %>%
  ggplot(aes(x = mean_value, y = proportion)) +
  geom_point() +
  labs(
    title = "Proportion of times the null was rejected (the power of the test) for each true value of μ",
    x = "True value of μ",
    y = "Power of the test")
```

Based on the plot, we can see that as the true value of μ increases from 1 to 5, the power of the test is also increasing. However, when the true value of μ increases from 5 ro 6, the power has almost no changes. Thus the conclusion would be that as sample size keeps the same, the higher the effect size is, the larger the power will be.


Make a plot showing the average estimate of μ^ on the y axis and the true value of μ on the x axis.

```{r}
sim_results_changes_df %>%
  group_by(mean_value) %>%
  summarize(
    average_estimate = mean(estimate)
  ) %>%
  ggplot(aes(x = mean_value, y = average_estimate)) +
  geom_point(color = "red") +
  labs(
    title = "Average estimate of μ^ for each true value of μ",
    x = "True value of μ",
    y = "Average estimate of μ^")
```

Make a second plot (or overlay on the first) the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ^ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

```{r}
sim_results_changes_df %>%
  mutate(
    rejection = case_when(
      p.value < 0.05 ~ 1, 
      p.value > 0.05 ~ 0
    )
  ) %>%
  filter(rejection == 1) %>%
  group_by(mean_value) %>%
  summarize(
    average_estimate_reject = mean(estimate)
  ) %>%
  ggplot(aes(x = mean_value, y = average_estimate_reject)) +
  geom_point(color = "blue") +
  labs(
    title = "Average estimate of μ^ only in samples for which the null was rejected for each true value of μ",
    x = "True value of μ",
    y = "Average estimate of μ^ rejected")
```

Based on the plots above, the sample average of μ^ across tests for which the null is rejected is not approximately equal to the true value of μ. Since the null hypothesis we are testing is H0:μ^=μ using α=0.05. As a result of test if at 95% confidence level there is sufficient evidence to reject the null, it is safe to conclude that μ^ is not equal to μ.
